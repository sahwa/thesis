\chapter{Introduction}

\section{Chromopainter and ancient DNA}

In this introduction I will outline the following: i) What are `haplotype-based' methods and what advantages and disadvantages do they offer over `unlinked' methods, ii) a summary of different methods used to analyse ancient DNA and iii) the need to merge datasets genotyped on different arrays and options for imputation. 

\subsection{Gains to be made with haplotype information}

Haplotype-based methods are those which explicitly model Linkage Disequilibrium (LD) between neighbouring SNPs along a haplotype. A `haplotype' is a sequence of alleles along chromosome. Note that other methods, for example \texttt{octopus} \cite{octopus} are referred to as `haplotype-based' genotype callers, but they represent a distinct group of methods to e.g. ChromoPainter.  

Linkage Disequilibrium (LD), the non-independence of alleles carried at different positions in the genome, has been studied since the earliest days of genetics \cite{morgan1912complete, bateson1902experiments} and has since been a fundamental aspect of virtually all areas of genetics. Interest in LD grew rapidly in the 1980s when its relevance to gene mapping was realised. Although understanding LD is of critical important to natural selection, gene conversion, mutation and other forces that cause a change in allele-frequency \cite{slatkin2008linkage}, here I will focus on its application to inference of haplotype sharing between individuals. 

The application of haplotype-based methods to research into human population structure inference was developed in the early 2000s \cite{conrad2006worldwide}. Although the study only used a small number of SNPs relative to the number used today, it revealed insights which are still relevant today, such as the presence of long haplotype blocks in highly drifted Native American populations. 
 
Accounting for recombination and LD within a model is necessarily computationally complex, as the number of combinations of alleles and their possible evolutionary histories balloons as the number of loci considered increases (does it scale quadratically?). Thus, the creation of the Li and Stephens copying model \cite{Li2003} was instrumental in the development of such methods \cite{song2016li} and provided an elegant solution to the increased complexity when using linked loci. As such, it is now a critical model in virtually all areas of genomic methodology, such as gene conversion parameters, admixed populations, human colonization history, local ancestry in admixed populations, imputation and haplotype estimation. 

Perhaps the first paper to formalise a haplotype-based approach for the study of population history was that of Hellenthal et al 2008 \cite{hellenthal2008inferring}. Building on the copying model proposed by Hellenthal et al, Lawson et al (2015) \cite{Lawson2012} created ChromoPainter, a model which explicitly accounts for linkage and variable recombination rates in order to find the genealogically closest haplotype to a target in a set of references. The authors showed that ChromoPainter had an enhanced ability to separate closely related populations when plotted on a PCA compared to unlinked methods. ChromoPainter was originally developed in tandem with its own clustering method fineSTRUCTURE, and has since been extended into methods to detect and date admixture \cite{Hellenthal2014}, and infer ancestry proportions \cite{Chacon-Duque2018}. 

ChromoPainter can be run in either `linked' or `unlinked' mode. In the linked mode, described in detail in later sections, LD between neighbouring SNPs is accounted for. `Unlinked' mode assumes a model of linkage equilibrium between markers and has been shown to be statistically identical to the likelihood model underlying the commonly used ADMIXTURE algorithm. 

A typical case study, and one which I will return to in later chapters, was a study which attempted to identify population structure among individuals from the British Isles. This study, hereafter referred to as POBI, genotyped 2039 people from England, Wales and Scotland. In summary, it was possible to detect structure down to the level of Devon and Cornwall (two neighbouring counties) using ChromoPainter. On the other hand, little structure was apparent when using unlinked methods (PCA). This outlines the benefits of incorporating linkage information when attempting to identify fine-scale structure between closely related populations.


It has since become a mainstay of population genetic research, both in humans and other organisms. It has also been incorporated into many ancient DNA studies, which I will discuss in the next section. 

However, the usage of haplotype-based methods is not without drawbacks. They are typically slower by several orders of magnitude, as the computational complexity is something. 

Secondly, the nature of haplotype-based methods means they require the data to be phased. Phasing is a statistical procedure \footnote{Phasing can also be performed using other methods, such as sequencing family trios. However, this is rarely used in population genetic studies and so I will not discuss it here} that requires substantial computation resources. Phasing is a procedure which is often error-prone (switch errors).


\section{Methods used to analyse ancient DNA}

The first ancient DNA papers mostly relied on statistical methods which compare allele-sharing or allele-frequencies between populations or individuals. These methods, in particular f-statistics and their extensions \cite{Green2010, Patterson2012, peter2016admixture} and Principle Component Analysis \cite{price2006principal}, can address a wide-range of questions pertaining, but not limited to, population structure, admixture, genetic similarity and population graphs. 

The early studies of ancient DNA generally considered broad-scale questions about human history, such as the nature of human-archaic interactions or the spread of farming technology across Europe \cite{Lazaridis2014}. Due to the infancy of the field, sample sizes were small, as efficient methods to obtain genetic data from ancient samples, such as SNP capture arrays, had yet to be developed. Early studies tended to look for genetic differences between populations which diverged many generations ago. For example, in the case of Lazaridis et al (2014), simply plotting Loschbour and Stuttgart on a PCA of modern individual showed they had substantially different ancestries, with Loschbour being most similar to present-day North-East Europeans, but falling well outside the variation of present-day Europeans, and Stuttgart clustering with present-day individuals from Tuscany. 

Such methods, which I will hereafter refer to as `unlinked', as they assume a model of linkage equilibrium (i.e. each SNP is independent of another in the context of) between neighbouring SNPs, are useful for ancient DNA studies.  Data is often represented in `psuedo-haploid' format to avoid making diploid genotype calls at low coverage positions; true heterozygous positions may be mis-called as homozgyous at low coverages, as there is a probability of $p()$ that no reads mapping to a particular allele may be sampled. Representing variants psuedo-haploid is used widely in past and current ancient DNA studies. It is also simple to account for missing data in PCA and f-statistics.    

However, in recent years, the low hanging fruit of broad-scale questions have mostly been answered and studies into more fine-scale populations structures have become more prevalent. Accordingly, methods which can detect more subtle population structure have been required. Include example here. 


Another reason for the use of alternate methods to $f_{3}$ statistics is that they possibly display bias in the face of drifted populations. 


The first use of chromopainter on ancient DNA was in a seminal paper from Lazaridis et al (2014) \cite{Lazaridis2014}. The two samples, Loschbour and Stuttgart, we of high coverage and therefore imputation was not used.

The first study to explicitly investigate the reliability of ChromoPainter on ancient DNA was Martiniano et al (2017) \cite{Martiniano2017}. They first tested the accuracy of imputation on ancient DNA samples by downsampling high coverage ancient genomes and comparing imputed to full-coverage non-imputed genotypes. Overall, they found a good correspondence. They also found that filtering the SNPs based on posterior genotype probabilities had little effect. 

As there is currently no way to allow missing positions ChromoPainter, imputation is necessary. Recently, Hui et al (2020) evaluated the accuracy of genotype imputation in ancient DNA samples \cite{hui2020evaluating}. 

As sample sizes used in ancient DNA studies has rapidly increased to >100, more analyses have incorporate haplotype-based methods. Each study typically carries out a small analysis to ensure that imputation in low-coverage ancient DNA samples is accurate. Antonio et al (2019) \cite{antonio2019ancient} analysed 127 ancient genomes of a mean coverage of 1x. To test imputation accuracy, they downsampled a single individual (NE1) to different levels of coverage and calculated the proportion of genotypes which matched the full coverage. However, this analysis was only performed on a single sample and the effect of imputation on the chromopainter process was not evaluated. As of writing (September 2021), the study of Margaryan et al (2020) is the biggest so far to use ChromoPainter, with over 400 samples used \cite{margaryan2020population}.

More recently, ChromoPainter has been used to study aspects of archaic hominin ancestry in present-day humans \cite{JACOBS20191010, teixeira2021widespread}. 

\section{Combining data from multiple chips}

A related issue stems from the current practice of developing a large number of genotyping arrays. Different cohorts are genotyped on different arrays and sets of SNPs, as different SNPs have different characteristics. For example, some SNPs are known to be associated with particular phenotypes, some SNPs are known to be more variable (and therefore more informative at identifying structure) in certain populations. Whilst this generation of custom genotyping arrays has meant a wider variety of questions and populations can be studied using genoyping arrays, it also makes combining data from across different arrays troublesome, as they often have a small overlap in positions.

For example, in my thesis, I have worked with at least 3 genotyping arrays; `Human Origins', `Hell Bus' and the UK Biobank. Often I have wanted to compare populations on different arrays, such as the African populations on the Human Origins array and UK Biobank individuals on the UK Biobank array. After merging the datasets, the overlap was small, only 70,000 SNPs. Having this few SNPs may result in a lack of power, so one option would be to impute the remaining SNPs using a reference panel. It is possible, however, that imputation may cause a bias towards particular populations in the reference panel. 

Therefore, one question to ask is the following; is it more desirable to impute the missing positions or to use use the overlapping SNPs. This is something which I will investigate in chapter 3.

\section{Issues with low coverage}

Coverage is an issue which has plagued the field of ancient DNA since its inception. Compared to DNA obtained from present-day samples, ancient DNA samples typically have a much lower proportion of endogenous DNA. This is because DNA degrades over time from environmental factors. Therefore, when the DNA fragments are sequenced, relatively few of them will align to the human reference. The coverage of a genome is therefore the mean number of reads mapped to each position in the genome. 

The primary issue with low-coverage data is the increased uncertainty when calling diploid genotypes, particularly when the true genotype is heterozygous. To mitigate this issue, many studies used `pseudo-haploid' genotyping, where a diploid genotype is reduced to a haploid by randomly sampling an allele from all the reads which have been aligned to that position in the genome. Whilst the use of pseudo-haploid calling eradicates the need to call heterozygous genotypes at low coverage positions, it also necessarily reduces the amount of information present at each position in the genome.

I will discuss in more detail later the effect of coverage on other methods. 
 
