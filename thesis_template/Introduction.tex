\chapter{Introduction}

\section{Chromopainter and ancient DNA}

In this introduction I will outline the following: i) What are `haplotype-based' methods and what advantages and disadvantages do they offer over `unlinked' methods, ii) a summary of different methods used to analyse ancient DNA and iii) the need to merge datasets genotyped on different arrays and options for imputation. 

\subsection{Gains to be made with haplotype information}

\subsubsection{History}

Haplotype-based methods are those which explicitly model Linkage Disequilibrium (LD) between neighbouring SNPs along a haplotype. This is in contrast to `unlinked' methods, which assume a model of Linkage Equilibrium between SNPS. A `haplotype' is a sequence of alleles along chromosome. Note that other methods, for example \texttt{octopus} \cite{octopus} are referred to as `haplotype-based' genotype callers, but they represent a distinct group of methods to e.g. ChromoPainter.  

Linkage Disequilibrium (LD), the non-independence of alleles carried at different positions in the genome, has been studied since the earliest days of genetics \cite{morgan1912complete, bateson1902experiments} and has since been a fundamental aspect of virtually all areas of genetics. In this thesis, here I will focus on its application to inference of haplotype sharing between individuals. 

Some of the earliest utilisation of LD information for the study of population history was by using microsatellite markers, whose linked repeats can be seen as analogous to linked alleles along a haplotype. Microsatellites were, and still are, commonly applied to study the population structure of wild animal systems; for instance, Amos et al (1993) used microsatellites to examine the population structure of whales \cite{amos1993social}. Later, LD  patterns at the CD4 locus were leveraged to show the preferred model of Human population history was a recent African origin \cite{tishkoff1996global}. 


The next major advance was the development of methods to use LD information between SNP markers rather than within microsatellites; studies in the early 2000s utilised the then-new Hap-Map results to show LD varies markedly across worldwide populations \cite{conrad2006worldwide}. This study calculated the proportion of unique haplotypes that were shared between two geographic regions, and by showing that the number of distinct haplotypes per region declines from Africa, provided additional evidence to the previously proposed recent African origin of humanity \cite{cann1987mitochondrial}. These findings were obtained despite the number of SNPs being very low by today's standards; only 2,834 autosomal SNPs were retained. This displays the power of using haplotype-based approaches and that the power scales nonlinearly with the total number of SNPs used.  

 
The 2000s saw a rapid increase in the number of SNP markers and individuals which had been sequenced. Accounting for recombination and LD within a model is necessarily computationally complex, as the number of combinations of alleles and their possible evolutionary histories balloons as the number of loci considered increases (does it scale quadratically?). Therefore, the new era of sequencing demanded new, more efficient methodology to cope with such data. The development of the Li and Stephens copying model (LSM) \cite{Li2003} was instrumental in the development of such methods \cite{song2016li} and provided an elegant solution to the increased complexity modeling recombination between linked loci. As such, it is now a critical model in virtually all areas of genomic methodology, such as gene conversion parameters, admixed populations, human colonization history, local ancestry in admixed populations and imputation. The LSM was, and still is, the foundation for methods of the haplotype phasing methods needed for haplotype-based methods \cite{stephens2003comparison, stephens2005accounting}. 

Perhaps the first paper to formalise a haplotype-based approach for the study of population history was that of Hellenthal et al 2008 \cite{hellenthal2008inferring}. The ancestry model of Hellenthal et al is based upon the LSM, using a Hidden Markov Model to reconstruct each target haploid as a mosaic of \textit{donor} haplotypes. The conditional (emission) probability that a given haploid `copies' from a particular reference haplotype is given by whether the alleles at the same position match, with switching between copying haplotypes informed by a genetic map. 

In the same year, Jakobsson et al (2008) analaysed a much larger number of SNPs (n=525,910) \cite{jakobsson2008genotype}. It was demonstrated that haplotype clusters show an elevated ability to determine local structure than unlinked SNPs alone;  51.87\% of haplotype clusters were found in at most two regions, in contrast with 4.66\% of SNP alleles. This seems naturally intuitive, as haplotype clusters are formed from different combination of SNP alleles, which are necessarily more unique than single SNPs. 


Building on the copying model proposed by Hellenthal et al (2008), Lawson et al (2015) \cite{Lawson2012} created ChromoPainter, a model which ...... The authors showed that ChromoPainter had an enhanced ability to separate closely related populations when plotted on a PCA compared to unlinked methods. ChromoPainter was originally developed in tandem with its own clustering method fineSTRUCTURE, and has since been extended into methods to detect and date admixture \cite{Hellenthal2014}, and infer ancestry proportions \cite{Chacon-Duque2018}. 

The `next-generation' of chromosome painting methods had to address the typical issue in population genomics; how to adapt methodology to larger and larger sample sizes; chromopainter was designed with datasets of <10,000 people in mind, whereas biobank-scale datasets typically contain 500,000+ individuals. One approach is to use the Burrows-Wheeler transform \cite{burrows1994block, DurbinPBWT}, which allows efficient matching of haplotypes in large datasets.

\subsubsection{Advantages of accounting for haplotypes}

ChromoPainter can be run in either `linked' or `unlinked' mode. In the linked mode, described in detail in later sections, LD between neighbouring SNPs is accounted for. `Unlinked' mode assumes a model of linkage equilibrium between markers and has been shown to be statistically identical to the likelihood model underlying the commonly used ADMIXTURE algorithm. 

A typical case study, and one which I will return to in later chapters, was a study which attempted to identify population structure among individuals from the British Isles. This study, hereafter referred to as POBI, genotyped 2039 people from England, Wales and Scotland. In summary, it was possible to detect structure down to the level of Devon and Cornwall (two neighbouring counties) using ChromoPainter. On the other hand, little structure was apparent when using unlinked methods (PCA). This outlines the benefits of incorporating linkage information when attempting to identify fine-scale structure between closely related populations.

Gattepaille and Jakobson (2012) \cite{JakobssonCombiningMarkers} provided the mathematical foundations for the advantage of using linked markers over unlinked ones. They describe a metric, $GIA$ (gain of informativeness for assignment), a term borrowed from information theory to describe the additional amount of information gained when using haplotype data instead of individuals alleles separately. They showed that whilst combining two markers is not necessarily advantageous for ancestry inference, $GIA$ is often positive for markers in LD with one another, demonstrating the advantage of haplotypes. Under a variety of simulated scenarios, incorrect assignment of individuals into populations was reduced between 26\% and 97\% when using haplotype data. They showed that using empirical data of individuals from France and Germany, accounting for haplotypes could reduce the rate of mis-assignment by 73\%. 

One less considered advantage of using haplotype information is that it may mitigate ascertainment bias. Ascertainment bias occurs when a subset of SNPs are chosen for analysis. SNPs are typically chosen because they display variation. If this variation is determined in one population, say British, then there is no guarantee that the variation will also be seen in another population, say Han Chinese. Therefore, including these SNPs can often provide misleading estimates of genetic diversity and commonly estimated parameters such as $F_{st}$ \cite{BergstromHGDP}. Conrad et al (2006) showed that, owing to the lack of African individuals used in the SNP discovery process, populations from the Middle East, Europe and South Asia showed the highest levels of heterozygosity. These findings were in stark disagreement with the currently accepted model of human history and studies which demonstrated Africans have the highest levels of genetic diversity \cite{rosenberg2002genetic, ramachandran2005support, bowcock1994high}. However, when instead of SNP heterozygosity, haplotype heterozygosity is used as a metric for diversity, African populations consistently have the highest values. The reason for this is, although the ascertainment for a particular SNP may depend strongly upon the ascertainment scheme, the same underlying haplotypes are likely to be observed, regardless of which SNPs are used to tag them. Thus, ascertainment is less likely to ascertain

In a similar manner, another advantage of using haplotype-based methods is that rare alleles are not required. Rare alleles are highly informative about recent, fine-scale population structure, as they are shared by the fewest number of individuals (max n=2) within a dataset. Methods which leverage this information have been used to model the population history of large datasets \cite{schiffels2016iron, gravel2011demographic, o2015rare}. However, rare alleles are harder to genotype, as they are more difficult to distinguish from sequencing errors. This is particularly the case when using relatively low-coverage genomes. Because of this, allele-frequency filters are often applied in population genetic studies. Further, more SNPs need to be sequenced in order to find rare variants in a wide range of populations. Using haplotype information negates the needs for using rare variants; if individuals share long haplotypes in common, then by default will also share rare variants that occur on those haplotypes. 

However, the usage of haplotype-based methods is not without drawbacks. They are typically slower by several orders of magnitude, as the computational complexity is something. 

Secondly, the nature of haplotype-based methods means they require the data to be phased. Phasing is a statistical procedure \footnote{Phasing can also be performed using other methods, such as sequencing family trios. However, this is rarely used in population genetic studies and so I will not discuss it here} that requires substantial computation resources. Phasing is a procedure which is often error-prone (switch errors). Care must be taken to ensure the appropriate samples are included in the reference panel


\section{Methods used to analyse ancient DNA}

Here, I will outline some of the most widely used methods to analyse ancient DNA.

\subsection{Unlinked methods}

The first ancient DNA papers mostly relied on statistical methods which compare allele-sharing or allele-frequencies between populations or individuals. These methods, in particular f-statistics and their extensions \cite{Green2010, Patterson2012, peter2016admixture, AssessingqpAdm} and Principle Component Analysis \cite{price2006principal}, can address a wide-range of questions pertaining, but not limited to, population structure, admixture, genetic similarity and population graphs. 

One reason why methods based on allele-sharing and allele-frequency differences are widely used in ancient DNA studies is that they can easily be modified to work with data in pseudo-haploid format. Pseudo-haploid genotypes are generated by sampling a read at random to represent a single allele at a given SNP. This is often necessary, because there are often not enough reads covering a SNP to confidently call heterozygous genotypes, and so are particularly suited to ancient DNA studies where there may only be a single read covering a SNP. Pseudo-haploid calls are therefore used widely, including currently (e.g. \cite{sirak2021social}), in most studies of ancient humans. 

Whilst pseudo-haploid genotype calls circumvent the problem of calling heterozgyous genotypes at low coverage positions, there is necessarily a reduction in the information they hold relative to true diploid genotypes and are thus less powerful. For many of the early ancient DNA studies, such as that of Green et al 2010 \cite{Green2010} and Lazaridis et al 2014 \cite{Lazaridis2014}, powerful methods for detecting population substructure and admixture were not required as they primarily considered broad-scale questions about human history, such as the nature of human-archaic interactions and whether there was significant genetic differences between the first farmers and the preceding hunter-gatherers. These populations, particularly humans and Neanderthals, are highly diverged and hence do not require powerful methods. For example, in the case of Lazaridis et al (2014), simply plotting Loschbour and Stuttgart on a PCA of modern individual showed they had substantially different ancestries.

Perhaps the most widely used method amenable to pseudo-haploid data is the family of F-statistics \footnote{Although related, they should not to be confused with Sewall Wright's F-statistics \cite{wright1949genetical}.}, which were first outlined in a 2009 study into the population history of India \cite{reich2009reconstructing}. These methods use the principle of shared drift in order to estimate genetic similarity ($f_{2}$), branch-length and admixture ($f_{3}$) and tests of treeness ($f_{4}$). Since 2009, F-statistics have been extended into multiple, more advanced, frameworks which are able to answer more complex questions about population history through the generation of population admixture graphs. In particular, qpAdm has been shown to be a flexible and coverage-robust method of estimating individual and population level admixture fractions \cite{AssessingqpAdm}. 

A similar method is the so-called ABBA-BABA test, developed by Green et al (2010) \cite{Green2010} in order to determine whether, and to what extent, admixture between humans and the newly sequenced Neanderthal genome had occurred. This simple test counts the number of times across the genome a 4 population phylogenetic tree shows a particular configuration at a given locus.   

\subsection{ChromoPainter ancient DNA}

In recent years, the `low hanging fruit' of broad-scale questions have mostly been answered and studies into more fine-scale populations structures have become more prevalent. Accordingly, methods which can detect more subtle population structure have been required. Include example here. 

The first use of chromopainter on ancient DNA was also in Lazaridis et al (2014) \cite{Lazaridis2014}. The two samples, Loschbour and Stuttgart, we of high coverage and therefore imputation was not used.

The first study to explicitly investigate the reliability of ChromoPainter on ancient DNA was Martiniano et al (2017) \cite{Martiniano2017}. They first tested the accuracy of imputation on ancient DNA samples by downsampling high coverage ancient genomes and comparing imputed to full-coverage non-imputed genotypes. Overall, they found a good correspondence. They also found that filtering the SNPs based on posterior genotype probabilities had little effect. 

As there is currently no way to allow missing positions ChromoPainter, imputation is necessary. Recently, Hui et al (2020) evaluated the accuracy of genotype imputation in ancient DNA samples \cite{hui2020evaluating}. 

As sample sizes used in ancient DNA studies has rapidly increased to >100, more analyses have incorporate haplotype-based methods. As more studies have incorporated them, 

Each study typically carries out a small analysis to ensure that imputation in low-coverage ancient DNA samples is accurate. Antonio et al (2019) \cite{antonio2019ancient} analysed 127 ancient genomes of a mean coverage of 1x. To test imputation accuracy, they downsampled a single individual (NE1) to different levels of coverage and calculated the proportion of genotypes which matched the full coverage. However, this analysis was only performed on a single sample and the effect of imputation on the chromopainter process was not evaluated. As of writing (September 2021), the study of Margaryan et al (2020) is the biggest so far to use ChromoPainter, with over 400 samples used \cite{margaryan2020population}.

More recently, ChromoPainter has been used to study aspects of archaic hominin ancestry in present-day humans \cite{JACOBS20191010, teixeira2021widespread}. Whilst not specifically designed to estimate local ancestry, it is possible to 


\section{Issues with low coverage}

Coverage is an issue which has plagued the field of ancient DNA since its inception. Compared to DNA obtained from present-day samples, ancient DNA samples typically have a much lower proportion of endogenous DNA. This is because DNA degrades over time from environmental factors. Therefore, when the DNA fragments are sequenced, relatively few of them will align to the human reference. The coverage of a genome is therefore the mean number of reads mapped to each position in the genome. 

The primary issue with low-coverage data is the increased uncertainty when calling diploid genotypes, particularly when the true genotype is heterozygous. To mitigate this issue, many studies used `pseudo-haploid' genotyping, where a diploid genotype is reduced to a haploid by randomly sampling an allele from all the reads which have been aligned to that position in the genome. Whilst the use of pseudo-haploid calling eradicates the need to call heterozygous genotypes at low coverage positions, it also necessarily reduces the amount of information present at each position in the genome.

I will discuss in more detail later the effect of coverage on other methods. 

\section{Combining data from multiple chips}

A related issue stems from the current practice of developing a large number of genotyping arrays. Different cohorts are genotyped on different arrays and sets of SNPs, as different SNPs have different characteristics. For example, some SNPs are known to be associated with particular phenotypes, some SNPs are known to be more variable (and therefore more informative at identifying structure) in certain populations. Whilst this generation of custom genotyping arrays has meant a wider variety of questions and populations can be studied using genoyping arrays, it also makes combining data from across different arrays potentially troublesome, as they often have a small overlap in the SNPs upon which they have been genotyped.

For example, in my thesis, I have worked with at least 3 genotyping arrays; `Human Origins', `Hell Bus' and the UK Biobank. Often I have wanted to compare populations on different arrays, such as the African populations on the Human Origins array and UK Biobank individuals on the UK Biobank array. After merging the datasets, the overlap was small, only 70,000 SNPs. This is around an order of magnitude fewer SNPs than a typical ChromoPainter analysis. 

Having a smaller number of SNPs may reduce power in two ways. Firstly, there is simply fewer informative data points to use when comparing the SNP patterns between two populations and therefore fewer possible data points which can be used to identify populations. Secondly, ChromoPainter derives part of its power from the LD between neighbouring SNPs. LD between two neighbouring SNPs is correlated with their physical distance. Fewer overall SNPs means each neighbouring pair of SNPs are physically further away from one another and thus have less LD information. 

One solution to the issue of a small number of SNP would be to impute the remaining SNPs. In this context, imputation refers to estimating missing genotypes using of a model usually based upon the LSM and a large reference panel. Imputation is widely used in e.g. GWAS studies to generate sequence-level data.

However, it is possible that imputation may cause a bias in the data. If missing genotypes are imputed incorrectly more often from one population than another, this will result in an increased, but spurious genetic similarity between the target and reference population. This may be a particular issue when analysing populations which are not well represented in imputation reference panels, such as non-Europeans. The nature and magnitude of this bias, however, is yet to be fully understood, particularly in the context of ChromoPainter.  

Therefore, one question to ask is the following; is it more desirable to impute the missing positions or to use use a smaller number of overlapping SNPs. This is something which I will investigate in chapter 3 with a case study investigating African ancestry in the UK Biobank dataset. 
 
