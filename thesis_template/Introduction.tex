\chapter{Introduction}

\section{Chromopainter and ancient DNA}

In this introduction I will outline the following: i) What are `haplotype-based' methods and what advantages and disadvantages do they offer over `unlinked' methods, ii) a summary of different methods used to analyse ancient DNA and 

\subsection{Gains to be made with haplotype information}

Here, I will define `haplotype-based' method as one which explicitly models linkage between neighbouring SNPs. Although LD has been studied since the earliest days of genetics (cite), its use in population structure inference was developed in the early 2000s \cite{conrad2006worldwide}.

Accounting for recombination and LD within a model is necessarily computationally complex, as the number of combinations of alleles and their associated evolutionary histories balloons as the number of loci considered increases (does it scale quadratically?). The Li and Stephens copying model was instrumental in the development of such methods \cite{song2016li} and provided an elegant solution to the increased complexity when using linked loci. As such, it is now a critical model in virtually all areas of genomic methodology, from imputation, phasing etc etc. 

Perhaps the first paper to formalise a haplotype-based approach was that of Hellenthal et al 2008 \cite{hellenthal2008inferring}.

Later, Lawson et al (2015) \cite{Lawson2012} developed what is now one of the leading approaches to modeling linkage between markers. This study showed that ChromoPainter had an enhanced ability to separate closely related populations when plotted on a PCA. ChromoPainter was originally developed in tandem with its own clustering method fineSTRUCTURE, and has since been extended into methods to detect and date admixture, and infer ancestry proportions. 

ChromoPainter can be run in either `linked' or `unlinked' mode. In the linked mode, described in detail in later sections, LD between neighbouring SNPs is accounted for. `Unlinked' mode assumes a model of linkage equilibrium between markers and has been shown to be statistically identical to the likelihood model underlying the commonly used ADMIXTURE algorithm. 

A typical case study, and one which I will return to in later chapters, was a study which attempted to identify population structure among individuals from the British Isles. This study, hereafter referred to as POBI, genotyped 2039 people from England, Wales and Scotland. In summary, it was possible to detect structure down to the level of Devon and Cornwall (two neighbouring counties) using ChromoPainter. On the other hand, little structure was apparent when using unlinked methods (PCA). This outlines the benefits of incorporating linkage information when attempting to identify fine-scale structure between closely related populations.

The gains from accounting for linkage between markers can be thought of as incorporating an additional spatial dimension into the data. 

It has since become a mainstay of population genetic research, both in humans and other organisms. It has also been incorporated into many ancient DNA studies, which I will discuss in the next section. 

However, the usage of haplotype-based methods is not without drawbacks. They are typically slower by several orders of magnitude, as the computational complexity is something. 

Secondly, the nature of haplotype-based methods means they require the data to be phased. Phasing is a statistical procedure \footnote{Phasing can also be performed using other methods, such as sequencing family trios. However, this is rarely used in population genetic studies and so I will not discuss it here} that requires substantial computation resources. Phasing is a procedure which is often error-prone (switch errors).

The first use of chromopainter on ancient DNA was in a seminal paper from Lazaridis et al (2014) \cite{Lazaridis2014}. The two samples, Loschbour and Stuttgart, we of high coverage and therefore imputation was not used.

Perhaps the first study to explicitly investigate chromopainter on ancient DNA was Martiniano et al (2017) \cite{Martiniano2017}. They first tested the accuracy of imputation on ancient DNA samples.


As sample sizes used in ancient DNA studies has rapidly increased to >100, more analyses have incorporate haplotype-based methods. Each study typically carries out a small analysis to ensure that imputation in low-coverage ancient DNA samples is accurate. Antonio et al (2019) \cite{antonio2019ancient} analysed 127 ancient genomes of a mean coverage of 1x. To test imputation accuracy, they downsampled a single individual (NE1) to different levels of coverage and calculated the proportion of genotypes which matched the full coverage. However, this analysis was only performed on a single sample and the effect of imputation on the chromopainter process was not evaluated. 

More recently, ChromoPainter has been used to study aspects of archaic hominin ancestry in present-day humans \cite{JACOBS20191010, teixeira2021widespread}. 

\section{Methods used to analyse ancient DNA}

The first ancient DNA papers mostly relied on statistical methods which compare allele-sharing or allele-frequencies between populations or individuals. These methods, in particular f-statistics and their extensions \cite{Green2010, Patterson2012, peter2016admixture} and Principle Component Analysis \cite{price2006principal}, can address a wide-range of questions pertaining, but not limited to, population structure, admixture, genetic similarity and population graphs. 

The early studies of ancient DNA generally considered broad-scale questions about human history, such as the nature of human-archaic interactions or the spread of farming technology across Europe \cite{Lazaridis2014}. Due to the infancy of the field, sample sizes were small, as efficient methods to obtain genetic data from ancient samples, such as SNP capture arrays, had yet to be developed. Early studies tended to look for genetic differences between populations which had been diverged for many generations. For example, in the case of Lazaridis et al (2014), simply plotting Loschbour and Stuttgart/LBK on a PCA of modern individual showed they had substantially different ancestries, with Loschbour being most similar to present-day North-East Europeans, but falling well outside the variation of present-day Europeans, and Stuttgart clustering with present-day individuals from Tuscany. 

These methods are suitable for such studies with

Such methods, which I will hereafter refer to as `unlinked', as they assume a model of linkage equilibrium between neighbouring SNPs, are useful for ancient DNA studies. Firstly, it is relatively easy to account for missingness in the data, by using e.g. a PCA projection. Second, you can use pseudo-haploid data with them. Lastly, something.

However, in recent years, the low hanging fruit of broad-scale questions have mostly been answered and studies into more fine-scale populations structures have become more prevalent. Accordingly, methods which can detect more subtle population structure.

Another reason for the use of alternate methods to $f_{3}$ statistics is that they possibly display bias in the face of drifted populations. 

\section{Combining data from multiple chips}

A related issue stems from the current practice of developing a large number of genotyping arrays. Different cohorts are genotyped on different arrays and sets of SNPs, as different SNPs have different characteristics. For example, some SNPs are known to be associated with particular phenotypes, some SNPs are known to be more variable (and therefore more informative at identifying structure) in certain populations. Whilst this generation of custom genotyping arrays has meant a wider variety of questions and populations can be studied using genoyping arrays, it also makes combining data from across different arrays troublesome, as they often have a small overlap in positions.

For example, in my thesis, I have worked with at least 3 genotyping arrays; `Human Origins', `Hell Bus' and the UK Biobank. Often I have wanted to compare populations on different arrays, such as the African populations on the Human Origins array and UK Biobank individuals on the UK Biobank array. After merging the datasets, the overlap was small, only 70,000 SNPs. Having this few SNPs may result in a lack of power, so one option would be to impute the remaining SNPs using a reference panel. It is possible, however, that imputation may cause a bias towards particular populations in the reference panel. 

Therefore, one question to ask is the following; is it more desirable to impute the missing positions or to use use the overlapping SNPs. This is something which I will investigate in chapter 3.

\section{Issues with low coverage}

Coverage is an issue which has plagued the field of ancient DNA since its inception. Compared to DNA obtained from present-day samples, ancient DNA samples typically have a much lower proportion of endogenous DNA. This is because DNA degrades over time from environmental factors. Therefore, when the DNA fragments are sequenced, relatively few of them will align to the human reference. The coverage of a genome is therefore the mean number of reads mapped to each position in the genome. 

The primary issue with low-coverage data is the increased uncertainty when calling diploid genotypes, particularly when the true genotype is heterozygous. To mitigate this issue, many studies used `pseudo-haploid' genotyping, where a diploid genotype is reduced to a haploid by randomly sampling an allele from all the reads which have been aligned to that position in the genome. Whilst the use of pseudo-haploid calling eradicates the need to call heterozygous genotypes at low coverage positions, it also necessarily reduces the amount of information present at each position in the genome.

I will discuss in more detail later the effect of coverage on other methods. 
 
